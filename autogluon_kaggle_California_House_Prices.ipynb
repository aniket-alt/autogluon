{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aniket-alt/Autogluon/blob/main/autogluon_kaggle_California_House_Prices.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPhQY707DOzK"
      },
      "source": [
        "# How to use AutoGluon for Kaggle competitions\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/autogluon/autogluon/blob/stable/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb)\n",
        "[![Open In SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/autogluon/autogluon/blob/stable/docs/tutorials/tabular/advanced/tabular-kaggle.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "This tutorial will teach you how to use AutoGluon to become a serious Kaggle competitor without writing lots of code.\n",
        "We first outline the general steps to use AutoGluon in Kaggle contests. Here, we assume the competition involves tabular data which are stored in one (or more) CSV files.\n",
        "\n",
        "1) Run Bash command: pip install kaggle\n",
        "\n",
        "2) Navigate to: https://www.kaggle.com/account and create an account (if necessary).\n",
        "Then , click on \"Create New API Token\" and move downloaded file to this location on your machine: `~/.kaggle/kaggle.json`. For troubleshooting, see [Kaggle API instructions](https://www.kaggle.com/docs/api).\n",
        "\n",
        "3) To download data programmatically: Execute this Bash command in your terminal:\n",
        "\n",
        "`kaggle competitions download -c [COMPETITION]`\n",
        "\n",
        "Here, [COMPETITION] should be replaced by the name of the competition you wish to enter.\n",
        "Alternatively, you can download data manually: Just navigate to website of the Kaggle competition you wish to enter, click \"Download All\", and accept the competition's terms.\n",
        "\n",
        "4) If the competition's training data is comprised of multiple CSV files, use [pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html) to properly merge/join them into a single data table where rows = training examples, columns = features.\n",
        "\n",
        "5) Run autogluon `fit()` on the resulting data table.\n",
        "\n",
        "6) Load the test dataset from competition (again making the necessary merges/joins to ensure it is in the exact same format as the training data table), and then call autogluon `predict()`.  Subsequently use [pandas.read_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) to load the competition's `sample_submission.csv` file into a DataFrame, put the AutoGluon predictions in the right column of this DataFrame, and finally save it as a CSV file via [pandas.to_csv](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html). If the competition does not offer a sample submission file, you will need to create the submission file yourself by appropriately reformatting AutoGluon's test predictions.\n",
        "\n",
        "7) Submit your predictions via Bash command:\n",
        "\n",
        "`kaggle competitions submit -c [COMPETITION] -f [FILE] -m [\"MESSAGE\"]`\n",
        "\n",
        "Here, [COMPETITION] again is the competition's name, [FILE] is the name of the CSV file you created with your predictions, and [\"MESSAGE\"] is a string message you want to record with this submitted entry. Alternatively, you can  manually upload your file of predictions on the competition website.\n",
        "\n",
        "8) Finally, navigate to competition leaderboard website to see how well your submission performed!\n",
        "It may take time for your submission to appear.\n",
        "\n",
        "\n",
        "\n",
        "Below, we demonstrate how to do steps (4)-(6) in Python for a specific Kaggle competition: [ieee-fraud-detection](https://www.kaggle.com/c/ieee-fraud-detection/).\n",
        "This means you'll need to run the above steps with `[COMPETITION]` replaced by `ieee-fraud-detection` in each command.  Here, we assume you've already completed steps (1)-(3) and the data CSV files are available on your computer. To begin step (4), we first load the competition's training data into Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLbYuHjGKawe",
        "outputId": "5d7b659e-0ad9-4de9-f35b-3f93dd621c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading california-house-prices.zip to /content\n",
            "\r  0% 0.00/29.5M [00:00<?, ?B/s]\n",
            "\r100% 29.5M/29.5M [00:00<00:00, 1.60GB/s]\n",
            "Archive:  /content/california-house-prices.zip\n",
            "  inflating: /content/california-house-prices/sample_submission.csv  \n",
            "  inflating: /content/california-house-prices/test.csv  \n",
            "  inflating: /content/california-house-prices/train.csv  \n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "\n",
        "!kaggle competitions download -c california-house-prices\n",
        "\n",
        "!unzip /content/california-house-prices.zip -d /content/california-house-prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mph8ode_KgGe",
        "outputId": "996c2e40-3964-40a3-b789-e710e18074db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.3\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-80.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "6d860dea459b4a239665b9fcd3c138ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon\n",
            "  Downloading autogluon-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.core==1.4.0 (from autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading autogluon.core-1.4.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting autogluon.features==1.4.0 (from autogluon)\n",
            "  Downloading autogluon.features-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.tabular==1.4.0 (from autogluon.tabular[all]==1.4.0->autogluon)\n",
            "  Downloading autogluon.tabular-1.4.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting autogluon.multimodal==1.4.0 (from autogluon)\n",
            "  Downloading autogluon.multimodal-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting autogluon.timeseries==1.4.0 (from autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading autogluon.timeseries-1.4.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<2.4.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.0.2)\n",
            "Requirement already satisfied: scipy<1.17,>=1.5.4 in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn<1.8.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.6.1)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.5)\n",
            "Requirement already satisfied: pandas<2.4.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.2.2)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.32.4)\n",
            "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.10.0)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading boto3-1.40.66-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting autogluon.common==1.4.0 (from autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading autogluon.common-1.4.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyarrow<21.0.0,>=7.0.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (18.1.0)\n",
            "Requirement already satisfied: psutil<7.1.0,>=5.7.3 in /usr/local/lib/python3.12/dist-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (5.9.5)\n",
            "Requirement already satisfied: joblib<1.7,>=1.2 in /usr/local/lib/python3.12/dist-packages (from autogluon.common==1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.5.2)\n",
            "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.12/dist-packages (from autogluon.core[all]==1.4.0->autogluon) (0.2.7)\n",
            "Collecting ray<2.45,>=2.10.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading ray-2.44.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: Pillow<12,>=10.0.1 in /usr/local/lib/python3.12/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (11.3.0)\n",
            "Collecting torch<2.8,>=2.2 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting lightning<2.8,>=2.2 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading lightning-2.5.5-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting transformers<4.50,>=4.38.0 (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: accelerate<2.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.11.0)\n",
            "Requirement already satisfied: fsspec<=2025.3 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (2025.3.0)\n",
            "Collecting jsonschema<4.24,>=4.18 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting timm<1.0.7,>=0.9.5 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading timm-1.0.3-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting torchvision<0.23.0,>=0.16.0 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: scikit-image<0.26.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.25.2)\n",
            "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.12/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (1.3)\n",
            "Collecting torchmetrics<1.8,>=1.2.0 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading torchmetrics-1.7.4-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: omegaconf<2.4.0,>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (2.3.0)\n",
            "Collecting pytorch-metric-learning<2.9,>=1.3.0 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: nltk<3.10,>=3.4.5 in /usr/local/lib/python3.12/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (3.9.1)\n",
            "Collecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (0.7.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.12/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (3.1.6)\n",
            "Requirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.12/dist-packages (from autogluon.multimodal==1.4.0->autogluon) (2.19.0)\n",
            "Collecting pytesseract<0.4,>=0.3.9 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-ml-py3<8.0,>=7.352.0 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2image<1.19,>=1.17.0 (from autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting catboost<1.3,>=1.2 (from autogluon.tabular[all]==1.4.0->autogluon)\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: fastai<2.9,>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (2.8.5)\n",
            "Collecting loguru (from autogluon.tabular[all]==1.4.0->autogluon)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: lightgbm<4.7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (4.6.0)\n",
            "Collecting einx (from autogluon.tabular[all]==1.4.0->autogluon)\n",
            "  Downloading einx-0.3.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting xgboost<3.1,>=2.0 (from autogluon.tabular[all]==1.4.0->autogluon)\n",
            "  Downloading xgboost-3.0.5-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: spacy<3.9 in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (3.8.7)\n",
            "Requirement already satisfied: huggingface-hub[torch] in /usr/local/lib/python3.12/dist-packages (from autogluon.tabular[all]==1.4.0->autogluon) (0.36.0)\n",
            "Collecting pytorch-lightning (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading pytorch_lightning-2.5.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting gluonts<0.17,>=0.15.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading gluonts-0.16.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting statsforecast<2.0.2,>=1.7.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading statsforecast-2.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (29 kB)\n",
            "Collecting mlforecast<0.15.0,>=0.14.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading mlforecast-0.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting utilsforecast<0.2.12,>=0.2.3 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading utilsforecast-0.2.11-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting coreforecast<0.0.17,>=0.0.12 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading coreforecast-0.0.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting fugue>=0.9.0 (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading fugue-0.9.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: orjson~=3.9 in /usr/local/lib/python3.12/dist-packages (from autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (3.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (6.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon) (0.6.2)\n",
            "Collecting botocore<1.41.0,>=1.40.66 (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading botocore-1.40.66-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.66->boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.66->boto3<2,>=1.10->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (0.21)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (1.17.0)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (4.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.4.0->autogluon) (0.70.16)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (25.3)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.9,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.8.13)\n",
            "Requirement already satisfied: fasttransform>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.0.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.12/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (1.0.3)\n",
            "Requirement already satisfied: plum-dispatch in /usr/local/lib/python3.12/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (2.6.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (3.1.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (3.13.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.12/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.11.10)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.12/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (4.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (1.0.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.4.0->autogluon) (0.10.9.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.4.0->autogluon) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.4.0->autogluon) (0.28.0)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (80.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.2.5)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.60.0)\n",
            "Collecting optuna (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting window-ops (from mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (5.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.4.0->autogluon) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk<3.10,>=3.4.5->autogluon.multimodal==1.4.0->autogluon) (2024.11.6)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4.0,>=2.1.1->autogluon.multimodal==1.4.0->autogluon) (4.9.3)\n",
            "Collecting colorama (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (13.9.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (3.20.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.1.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.12/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (5.29.5)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.12/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.4.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.12/dist-packages (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.8.0)\n",
            "Collecting aiohttp_cors (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading colorful-0.5.8-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting py-spy>=0.4.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (510 bytes)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.12/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.76.0)\n",
            "Collecting opencensus (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.23.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.12/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (7.4.1)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading virtualenv-20.35.4-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (2025.10.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.4.0->autogluon) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<1.8.0,>=1.4.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon) (3.6.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.20.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.14.5)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.4.0->autogluon) (3.1.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.13.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.80)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.5.4.2)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.11.1.6)\n",
            "Collecting triton==3.3.1 (from torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers<4.50,>=4.38.0->transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub[torch]->autogluon.tabular[all]==1.4.0->autogluon) (1.2.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.12/dist-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.4.0->autogluon) (0.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (0.23.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (2.6.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3->autogluon.multimodal==1.4.0->autogluon) (1.22.0)\n",
            "Collecting triad>=1.0.0 (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading triad-1.0.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting adagio>=0.2.6 (from fugue>=0.9.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (4.13.5)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.4.0->autogluon) (1.3.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (0.43.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon) (0.1.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<2.8,>=2.2->autogluon.multimodal==1.4.0->autogluon) (1.3.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading distlib-0.4.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (4.5.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (2.8)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from einx->autogluon.tabular[all]==1.4.0->autogluon) (2.4.6)\n",
            "Collecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.28.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.71.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (2.38.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (4.9.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon) (0.6.1)\n",
            "Collecting pycryptodome (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading openxlab-0.1.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting filelock (from ray<2.45,>=2.10.0->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting packaging>=20.0 (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pytz>=2020.1 (from pandas<2.4.0,>=2.0.0->autogluon.core==1.4.0->autogluon.core[all]==1.4.0->autogluon)\n",
            "  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.4.0->autogluon)\n",
            "  Downloading openxlab-0.1.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.17.0)\n",
            "Collecting colorlog (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (2.0.44)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (1.3.10)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->mlforecast<0.15.0,>=0.14.0->autogluon.timeseries==1.4.0->autogluon.timeseries[all]==1.4.0->autogluon) (3.2.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.4.0->autogluon) (8.5.0)\n",
            "Requirement already satisfied: beartype>=0.16.2 in /usr/local/lib/python3.12/dist-packages (from plum-dispatch->fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.4.0->autogluon) (0.22.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.4.0->autogluon) (1.7.1)\n",
            "Downloading autogluon-1.4.0-py3-none-any.whl (9.8 kB)\n",
            "Downloading autogluon.core-1.4.0-py3-none-any.whl (225 kB)\n",
            "Downloading autogluon.common-1.4.0-py3-none-any.whl (70 kB)\n",
            "Downloading autogluon.features-1.4.0-py3-none-any.whl (64 kB)\n",
            "Downloading autogluon.multimodal-1.4.0-py3-none-any.whl (454 kB)\n",
            "Downloading autogluon.tabular-1.4.0-py3-none-any.whl (487 kB)\n",
            "Downloading autogluon.timeseries-1.4.0-py3-none-any.whl (189 kB)\n",
            "Downloading boto3-1.40.66-py3-none-any.whl (139 kB)\n",
            "Downloading botocore-1.40.66-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m154.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coreforecast-0.0.16-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
            "Downloading gluonts-0.16.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
            "Downloading lightning-2.5.5-py3-none-any.whl (828 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m828.5/828.5 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading mlforecast-0.14.0-py3-none-any.whl (71 kB)\n",
            "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "Downloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
            "Downloading ray-2.44.1-cp312-cp312-manylinux2014_x86_64.whl (68.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "Downloading statsforecast-2.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (353 kB)\n",
            "Downloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.0/821.0 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.4-py3-none-any.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.5/963.5 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp312-cp312-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m139.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utilsforecast-0.2.11-py3-none-any.whl (41 kB)\n",
            "Downloading xgboost-3.0.5-py3-none-manylinux_2_28_x86_64.whl (94.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fugue-0.9.2-py3-none-any.whl (280 kB)\n",
            "Downloading adagio-0.2.6-py3-none-any.whl (19 kB)\n",
            "Downloading py_spy-0.4.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "Downloading triad-1.0.0-py3-none-any.whl (59 kB)\n",
            "Downloading virtualenv-20.35.4-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.4.0-py2.py3-none-any.whl (469 kB)\n",
            "Downloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading colorful-0.5.8-py2.py3-none-any.whl (201 kB)\n",
            "Downloading einx-0.3.0-py3-none-any.whl (102 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "Downloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Downloading openxlab-0.0.11-py3-none-any.whl (55 kB)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "Downloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.5-py3-none-any.whl (832 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.4/832.4 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: nvidia-ml-py3, seqeval\n",
            "  Building wheel for nvidia-ml-py3 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19208 sha256=71a62572ba4113c77922c8df99e0a618bd0b35421512195aebec9475f7d03873\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/65/79/33dee66cba26e8204801916dfee7481bccfd22905ebb841fe5\n",
            "  Building wheel for seqeval (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16250 sha256=1bbd9e30b4fcf0a0368ace5d5c09ae744495d3ccb1ac1691e80c2c0354661484\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/b8/73/0b2c1a76b701a677653dd79ece07cfabd7457989dbfbdcd8d7\n",
            "Successfully built nvidia-ml-py3 seqeval\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py3, nvidia-cusparselt-cu12, distlib, colorful, virtualenv, triton, tensorboardX, pytesseract, pycryptodome, pdf2image, ordered-set, openxlab, nvidia-nccl-cu12, nvidia-cudnn-cu12, loguru, lightning-utilities, jmespath, coreforecast, colorlog, colorama, xgboost, window-ops, model-index, einx, botocore, utilsforecast, triad, torch, tokenizers, seqeval, s3transfer, optuna, opendatalab, jsonschema, gluonts, catboost, aiohttp_cors, transformers, torchvision, torchmetrics, ray, pytorch-metric-learning, openmim, opencensus, nlpaug, mlforecast, boto3, adagio, timm, pytorch-lightning, fugue, evaluate, autogluon.common, statsforecast, lightning, autogluon.features, autogluon.core, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.7.1\n",
            "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.7.1:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.7.1\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.4.0\n",
            "\u001b[2K    Uninstalling triton-3.4.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.4.0\n",
            "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "\u001b[2K  Attempting uninstall: xgboost\n",
            "\u001b[2K    Found existing installation: xgboost 3.1.1\n",
            "\u001b[2K    Uninstalling xgboost-3.1.1:\n",
            "\u001b[2K      Successfully uninstalled xgboost-3.1.1\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.8.0+cu126\n",
            "\u001b[2K    Uninstalling torch-2.8.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torch-2.8.0+cu126\n",
            "\u001b[2K  Attempting uninstall: tokenizers\n",
            "\u001b[2K    Found existing installation: tokenizers 0.22.1\n",
            "\u001b[2K    Uninstalling tokenizers-0.22.1:\n",
            "\u001b[2K      Successfully uninstalled tokenizers-0.22.1\n",
            "\u001b[2K  Attempting uninstall: jsonschema\n",
            "\u001b[2K    Found existing installation: jsonschema 4.25.1\n",
            "\u001b[2K    Uninstalling jsonschema-4.25.1:\n",
            "\u001b[2K      Successfully uninstalled jsonschema-4.25.1\n",
            "\u001b[2K  Attempting uninstall: transformers\n",
            "\u001b[2K    Found existing installation: transformers 4.57.1\n",
            "\u001b[2K    Uninstalling transformers-4.57.1:\n",
            "\u001b[2K      Successfully uninstalled transformers-4.57.1\n",
            "\u001b[2K  Attempting uninstall: torchvision\n",
            "\u001b[2K    Found existing installation: torchvision 0.23.0+cu126\n",
            "\u001b[2K    Uninstalling torchvision-0.23.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.23.0+cu126\n",
            "\u001b[2K  Attempting uninstall: timm\n",
            "\u001b[2K    Found existing installation: timm 1.0.21\n",
            "\u001b[2K    Uninstalling timm-1.0.21:\n",
            "\u001b[2K      Successfully uninstalled timm-1.0.21\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63/63\u001b[0m [autogluon]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed adagio-0.2.6 aiohttp_cors-0.8.1 autogluon-1.4.0 autogluon.common-1.4.0 autogluon.core-1.4.0 autogluon.features-1.4.0 autogluon.multimodal-1.4.0 autogluon.tabular-1.4.0 autogluon.timeseries-1.4.0 boto3-1.40.66 botocore-1.40.66 catboost-1.2.8 colorama-0.4.6 colorful-0.5.8 colorlog-6.10.1 coreforecast-0.0.16 distlib-0.4.0 einx-0.3.0 evaluate-0.4.6 fugue-0.9.2 gluonts-0.16.2 jmespath-1.0.1 jsonschema-4.23.0 lightning-2.5.5 lightning-utilities-0.15.2 loguru-0.7.3 mlforecast-0.14.0 model-index-0.1.11 nlpaug-1.1.11 nvidia-cudnn-cu12-9.5.1.17 nvidia-cusparselt-cu12-0.6.3 nvidia-ml-py3-7.352.0 nvidia-nccl-cu12-2.26.2 opencensus-0.11.4 opencensus-context-0.1.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optuna-4.5.0 ordered-set-4.1.0 pdf2image-1.17.0 py-spy-0.4.1 pycryptodome-3.23.0 pytesseract-0.3.13 pytorch-lightning-2.5.5 pytorch-metric-learning-2.8.1 ray-2.44.1 s3transfer-0.14.0 seqeval-1.2.2 statsforecast-2.0.1 tensorboardX-2.6.4 timm-1.0.3 tokenizers-0.21.4 torch-2.7.1 torchmetrics-1.7.4 torchvision-0.22.1 transformers-4.49.0 triad-1.0.0 triton-3.3.1 utilsforecast-0.2.11 virtualenv-20.35.4 window-ops-0.0.15 xgboost-3.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -U pip\n",
        "!pip3 install -U setuptools wheel\n",
        "\n",
        "!pip3 install autogluon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7O1z-BoNKm5z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "directory = './california-house-prices/'  # directory where you have downloaded the data CSV files from the competition\n",
        "# The label column for this dataset is 'median_house_value'\n",
        "label = 'Sold Price'\n",
        "eval_metric = 'rmse'  # Optional: specify that competition evaluation metric is RMSE for regression tasks\n",
        "save_path = directory + 'AutoGluonModels/'  # where to store trained models\n",
        "\n",
        "train_data = pd.read_csv(directory+'train.csv')\n",
        "test_data = pd.read_csv(directory+'test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN_k-cBSMOae"
      },
      "source": [
        "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit( train_data, presets='best_quality', time_limit=300)\n",
        "\n",
        "results = predictor.fit_summary()\n",
        "\n",
        "Now, we use the trained AutoGluon Predictor to make predictions on the competition's test data. It is imperative that multiple test data files are joined together in the exact same manner as the training data. Because this competition is evaluated based on the AUC (Area under the ROC curve) metric, we ask AutoGluon for predicted class-probabilities rather than class predictions. In general, when to use `predict` vs `predict_proba` will depend on the particular competition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i_7pmcqMifc",
        "outputId": "2b34fae6-7fe2-443a-dbc8-d8c45c255ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Verbosity: 3 (Detailed Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.4.0\n",
            "Python Version:     3.12.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP Thu Oct  2 10:42:05 UTC 2025\n",
            "CPU Count:          12\n",
            "GPU Count:          1\n",
            "Memory Avail:       50.99 GB / 52.96 GB (96.3%)\n",
            "Disk Space Avail:   189.98 GB / 235.68 GB (80.6%)\n",
            "===================================================\n",
            "Presets specified: ['medium_quality']\n",
            "============ fit kwarg info ============\n",
            "User Specified kwargs:\n",
            "{'auto_stack': False}\n",
            "Full kwargs:\n",
            "{'_experimental_dynamic_hyperparameters': False,\n",
            " '_feature_generator_kwargs': None,\n",
            " '_save_bag_folds': None,\n",
            " 'ag_args': None,\n",
            " 'ag_args_ensemble': None,\n",
            " 'ag_args_fit': None,\n",
            " 'auto_stack': False,\n",
            " 'calibrate': 'auto',\n",
            " 'delay_bag_sets': False,\n",
            " 'ds_args': {'clean_up_fits': True,\n",
            "             'detection_time_frac': 0.25,\n",
            "             'enable_callbacks': False,\n",
            "             'enable_ray_logging': True,\n",
            "             'holdout_data': None,\n",
            "             'holdout_frac': 0.1111111111111111,\n",
            "             'memory_safe_fits': True,\n",
            "             'n_folds': 2,\n",
            "             'n_repeats': 1,\n",
            "             'validation_procedure': 'holdout'},\n",
            " 'excluded_model_types': None,\n",
            " 'feature_generator': 'auto',\n",
            " 'feature_prune_kwargs': None,\n",
            " 'holdout_frac': None,\n",
            " 'hyperparameter_tune_kwargs': None,\n",
            " 'included_model_types': None,\n",
            " 'keep_only_best': False,\n",
            " 'learning_curves': False,\n",
            " 'name_suffix': None,\n",
            " 'num_bag_folds': None,\n",
            " 'num_bag_sets': None,\n",
            " 'num_stack_levels': None,\n",
            " 'pseudo_data': None,\n",
            " 'raise_on_model_failure': False,\n",
            " 'raise_on_no_models_fitted': True,\n",
            " 'refit_full': False,\n",
            " 'save_bag_folds': None,\n",
            " 'save_space': False,\n",
            " 'set_best_to_refit_full': False,\n",
            " 'test_data': None,\n",
            " 'unlabeled_data': None,\n",
            " 'use_bag_holdout': False,\n",
            " 'verbosity': 3}\n",
            "========================================\n",
            "Using hyperparameters preset: hyperparameters='default'\n",
            "Saving /content/california-house-prices/AutoGluonModels/learner.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/predictor.pkl\n",
            "Beginning AutoGluon training ... Time limit = 600s\n",
            "AutoGluon will save models to \"/content/california-house-prices/AutoGluonModels\"\n",
            "Train Data Rows:    47439\n",
            "Train Data Columns: 40\n",
            "Label Column:       Sold Price\n",
            "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == float and many unique label-values observed).\n",
            "\tLabel info (max, min, mean, stddev): (90000000.0, 100500.0, 1296050.49915, 1694452.20335)\n",
            "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    52075.79 MB\n",
            "\tTrain Data (Original)  Memory Usage: 95.74 MB (0.2% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n",
            "\t\t\tOriginal Features (exact raw dtype, raw dtype):\n",
            "\t\t\t\t('float64', 'float') : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int64', 'int')     :  2 | ['Id', 'Zip']\n",
            "\t\t\t\t('object', 'object') : 21 | ['Address', 'Summary', 'Type', 'Heating', 'Cooling', ...]\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])                      : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])                        :  2 | ['Id', 'Zip']\n",
            "\t\t\t\t('object', [])                     :  4 | ['Type', 'Region', 'City', 'State']\n",
            "\t\t\t\t('object', ['datetime_as_object']) :  2 | ['Listed On', 'Last Sold On']\n",
            "\t\t\t\t('object', ['text'])               : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])                      : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])                        :  2 | ['Id', 'Zip']\n",
            "\t\t\t\t('int', ['bool'])                  :  1 | ['State']\n",
            "\t\t\t\t('object', [])                     :  3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t('object', ['datetime_as_object']) :  2 | ['Listed On', 'Last Sold On']\n",
            "\t\t\t\t('object', ['text'])               : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\t0.1s = Fit runtime\n",
            "\t\t\t40 features in original data used to generate 40 features in processed data.\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])                      : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])                        :  2 | ['Id', 'Zip']\n",
            "\t\t\t\t('int', ['bool'])                  :  1 | ['State']\n",
            "\t\t\t\t('object', [])                     :  3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t('object', ['datetime_as_object']) :  2 | ['Listed On', 'Last Sold On']\n",
            "\t\t\t\t('object', ['text'])               : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])                      : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])                        :  2 | ['Id', 'Zip']\n",
            "\t\t\t\t('int', ['bool'])                  :  1 | ['State']\n",
            "\t\t\t\t('object', [])                     :  3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t('object', ['datetime_as_object']) :  2 | ['Listed On', 'Last Sold On']\n",
            "\t\t\t\t('object', ['text'])               : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\t0.1s = Fit runtime\n",
            "\t\t\t40 features in original data used to generate 40 features in processed data.\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])     : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])       :  2 | ['Id', 'Zip']\n",
            "\t\t\t\t('int', ['bool']) :  1 | ['State']\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('float', [])     : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])       :  2 | ['Id', 'Zip']\n",
            "\t\t\t\t('int', ['bool']) :  1 | ['State']\n",
            "\t\t\t0.0s = Fit runtime\n",
            "\t\t\t20 features in original data used to generate 20 features in processed data.\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t\t('category', [])                   :  3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t\t('category', ['text_as_category']) : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t\t('category', [])                   :  3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t\t('category', ['text_as_category']) : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\t\t0.0s = Fit runtime\n",
            "\t\t\t\t18 features in original data used to generate 18 features in processed data.\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('object', [])       :  3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t('object', ['text']) : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', [])                   :  3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t('category', ['text_as_category']) : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\t0.4s = Fit runtime\n",
            "\t\t\t18 features in original data used to generate 18 features in processed data.\n",
            "\t\tFitting DatetimeFeatureGenerator...\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('object', ['datetime_as_object']) : 2 | ['Listed On', 'Last Sold On']\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('int', ['datetime_as_int']) : 10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "\t\t\t0.1s = Fit runtime\n",
            "\t\t\t2 features in original data used to generate 10 features in processed data.\n",
            "\t\tFitting TextSpecialFeatureGenerator...\n",
            "\t\t\tFitting BinnedFeatureGenerator...\n",
            "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t\t('float', ['text_special']) : 110 | ['Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', 'Address.special_ratio', 'Address.symbol_ratio.#', ...]\n",
            "\t\t\t\t\t('int', ['text_special'])   :  80 | ['Address.char_count', 'Address.word_count', 'Address.symbol_count.#', 'Address.symbol_count. ', 'Summary.char_count', ...]\n",
            "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t\t('int', ['binned', 'text_special']) : 190 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t\t\t15.3s = Fit runtime\n",
            "\t\t\t\t190 features in original data used to generate 190 features in processed data.\n",
            "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\t\t\t\t10 duplicate columns removed: ['Address.symbol_count. ', 'Cooling features.digit_ratio', 'Parking.symbol_count. ', 'Parking features.symbol_count./', 'Middle School.digit_ratio', 'High School.digit_ratio', 'Flooring.digit_ratio', 'Heating features.digit_ratio', 'Middle School.symbol_count. ', 'High School.symbol_count. ']\n",
            "\t\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t\t('int', ['binned', 'text_special']) : 180 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t\t('int', ['binned', 'text_special']) : 180 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t\t\t2.1s = Fit runtime\n",
            "\t\t\t\t180 features in original data used to generate 180 features in processed data.\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('object', ['text']) : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('int', ['binned', 'text_special']) : 180 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t\t31.9s = Fit runtime\n",
            "\t\t\t15 features in original data used to generate 180 features in processed data.\n",
            "\t\tFitting TextNgramFeatureGenerator...\n",
            "\t\t\tFitting CountVectorizer for text features: ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', 'Bedrooms', 'Elementary School', 'Middle School', 'High School', 'Flooring', 'Heating features', 'Cooling features', 'Appliances included', 'Laundry features', 'Parking features']\n",
            "\t\t\t\tCountVectorizer(dtype=<class 'numpy.uint8'>, max_features=10000, min_df=30,\n",
            "                ngram_range=(1, 3))\n",
            "\t\t\tCountVectorizer fit with vocabulary size = 10000\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('object', ['text']) : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('int', ['text_ngram']) : 10001 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in august', ...]\n",
            "\t\t\t45.4s = Fit runtime\n",
            "\t\t\t15 features in original data used to generate 10001 features in processed data.\n",
            "\t\tSkipping IdentityFeatureGenerator: No input feature with required dtypes.\n",
            "\t\tSkipping IsNanFeatureGenerator: No input feature with required dtypes.\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', [])                    :     3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t('category', ['text_as_category'])  :    15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\t\t('float', [])                       :    17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])                         :     2 | ['Id', 'Zip']\n",
            "\t\t\t\t('int', ['binned', 'text_special']) :   179 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t\t\t('int', ['bool'])                   :     1 | ['State']\n",
            "\t\t\t\t('int', ['datetime_as_int'])        :    10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "\t\t\t\t('int', ['text_ngram'])             : 10001 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in august', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', [])                    :     3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t('category', ['text_as_category'])  :    15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\t\t('float', [])                       :    17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])                         :     2 | ['Id', 'Zip']\n",
            "\t\t\t\t('int', ['binned', 'text_special']) :   179 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t\t\t('int', ['bool'])                   :     1 | ['State']\n",
            "\t\t\t\t('int', ['datetime_as_int'])        :    10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "\t\t\t\t('int', ['text_ngram'])             : 10001 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in august', ...]\n",
            "\t\t\t17.1s = Fit runtime\n",
            "\t\t\t10228 features in original data used to generate 10228 features in processed data.\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\t\t\t1092 duplicate columns removed: ['__nlp__.burroughs middle school', '__nlp__.galileo high', '__nlp__.galileo high school', '__nlp__.in october 2020', '__nlp__.john burroughs', '__nlp__.john burroughs middle', '__nlp__.muir middle', '__nlp__.muir middle school', '__nlp__.october 2020', '__nlp__.school galileo', '__nlp__.school galileo high', '__nlp__.teresa', '__nlp__.window wall unit', '__nlp__.you ll find', '__nlp__.gas electric dryer', '__nlp__.los altos high', '__nlp__.school joseph', '__nlp__.the line appliances', '__nlp__.view elementary school', '__nlp__.abbott middle school', '__nlp__.bethune middle', '__nlp__.bethune middle school', '__nlp__.charles drew middle', '__nlp__.chavez', '__nlp__.compton high school', '__nlp__.convenient access to', '__nlp__.drew', '__nlp__.drew middle', '__nlp__.drew middle school', '__nlp__.elementary school abbott', '__nlp__.elementary school dartmouth', '__nlp__.gardner street elementary', '__nlp__.george middle school', '__nlp__.hollywood elementary school', '__nlp__.home is 300', '__nlp__.in desk', '__nlp__.in january 2020', '__nlp__.is 300 mo', '__nlp__.is centrally located', '__nlp__.january 2020', '__nlp__.kitchen family room', '__nlp__.land home', '__nlp__.large center island', '__nlp__.love with', '__nlp__.mary mcleod', '__nlp__.mary mcleod bethune', '__nlp__.mcleod', '__nlp__.mcleod bethune', '__nlp__.mcleod bethune middle', '__nlp__.middle school scotts', '__nlp__.opener gated', '__nlp__.opener on', '__nlp__.opener on site', '__nlp__.range hood inside', '__nlp__.rmks', '__nlp__.school abbott', '__nlp__.school abbott middle', '__nlp__.school compton', '__nlp__.school compton high', '__nlp__.school dartmouth', '__nlp__.school dartmouth middle', '__nlp__.school francisco', '__nlp__.school francisco middle', '__nlp__.school live oak', '__nlp__.school rancho', '__nlp__.scotts valley high', '__nlp__.scotts valley middle', '__nlp__.stone forced air', '__nlp__.vacant land home', '__nlp__.west hollywood elementary', '__nlp__.westmont high', '__nlp__.window unit dishwasher', '__nlp__.you ll love', '__nlp__.you ve been', '__nlp__.benjamin franklin senior', '__nlp__.borel middle', '__nlp__.capuchino high', '__nlp__.capuchino high school', '__nlp__.carport off street', '__nlp__.central air community', '__nlp__.elementary school capuchino', '__nlp__.foster city', '__nlp__.franklin senior', '__nlp__.franklin senior high', '__nlp__.gas hook up', '__nlp__.has its own', '__nlp__.law', '__nlp__.mateo high school', '__nlp__.san mateo high', '__nlp__.school adrian', '__nlp__.school adrian wilcox', '__nlp__.school capuchino', '__nlp__.school capuchino high', '__nlp__.school mountain view', '__nlp__.school san mateo', '__nlp__.school wallenberg raoul', '__nlp__.woodrow wilson', '__nlp__.school willow glen', '__nlp__.an en suite', '__nlp__.brighton middle school', '__nlp__.charter high school', '__nlp__.elementary school new', '__nlp__.everett middle', '__nlp__.everett middle school', '__nlp__.february 2020 the', '__nlp__.hillsdale high school', '__nlp__.in cabinetry', '__nlp__.jefferson high school', '__nlp__.merritt trace elementary', '__nlp__.new brighton middle', '__nlp__.room family room', '__nlp__.sac in', '__nlp__.school benjamin', '__nlp__.school benjamin franklin', '__nlp__.school everett', '__nlp__.school everett middle', '__nlp__.school monroe', '__nlp__.school new', '__nlp__.school new brighton', '__nlp__.school venice senior', '__nlp__.see remarks garage', '__nlp__.take advantage of', '__nlp__.trace elementary', '__nlp__.want to miss', '__nlp__.wilson senior', '__nlp__.wilson senior high', '__nlp__.woodrow wilson senior', '__nlp__.air direct access', '__nlp__.boulevard elementary school', '__nlp__.central air direct', '__nlp__.crittenden middle', '__nlp__.crittenden middle school', '__nlp__.deemed reliable but', '__nlp__.elementary school crittenden', '__nlp__.furnace ceiling fan', '__nlp__.gas multi', '__nlp__.gas multi zone', '__nlp__.ins and', '__nlp__.is filled with', '__nlp__.its finest', '__nlp__.january 2020 the', '__nlp__.laurel elementary school', '__nlp__.milpitas middle school', '__nlp__.oaks elementary school', '__nlp__.outdoor flow', '__nlp__.price charter', '__nlp__.price charter middle', '__nlp__.quick access to', '__nlp__.rancho milpitas middle', '__nlp__.reliable', '__nlp__.reliable but', '__nlp__.responsible for', '__nlp__.rio rico', '__nlp__.sac location', '__nlp__.school bret harte', '__nlp__.school charles', '__nlp__.school charles drew', '__nlp__.school crittenden', '__nlp__.school crittenden middle', '__nlp__.school george washington', '__nlp__.school rancho milpitas', '__nlp__.school westmont', '__nlp__.school westmont high', '__nlp__.spa like bath', '__nlp__.the sought after', '__nlp__.tops stainless', '__nlp__.van ness', '__nlp__.westmont high school', '__nlp__.advantage of', '__nlp__.forced air stove', '__nlp__.garage off street', '__nlp__.glen high', '__nlp__.glen high school', '__nlp__.glen middle', '__nlp__.glen middle school', '__nlp__.in august', '__nlp__.in august 2020', '__nlp__.middle school willow', '__nlp__.move in condition', '__nlp__.none forced air', '__nlp__.school audubon', '__nlp__.school audubon middle', '__nlp__.school martin murphy', '__nlp__.silver creek high', '__nlp__.variety of', '__nlp__.willow glen high', '__nlp__.willow glen middle', '__nlp__.220v electricity hookup', '__nlp__.aptos middle school', '__nlp__.attached interior access', '__nlp__.central natural gas', '__nlp__.floor washer dryer', '__nlp__.forced air multi', '__nlp__.francisco high school', '__nlp__.francisco middle school', '__nlp__.fresco dining', '__nlp__.gunderson high', '__nlp__.gunderson high school', '__nlp__.has plenty of', '__nlp__.home is 600', '__nlp__.hookup 220v electricity', '__nlp__.hookup electricity', '__nlp__.hookup electricity hookup', '__nlp__.ida high school', '__nlp__.in garage gas', '__nlp__.is 600 mo', '__nlp__.john liechty middle', '__nlp__.kenter canyon', '__nlp__.liechty', '__nlp__.liechty middle', '__nlp__.liechty middle school', '__nlp__.mature fruit trees', '__nlp__.middle school gunderson', '__nlp__.middle school wells', '__nlp__.oak high school', '__nlp__.open concept kitchen', '__nlp__.opener guest', '__nlp__.range hood in', '__nlp__.san francisco high', '__nlp__.school aptos middle', '__nlp__.school gunderson', '__nlp__.school gunderson high', '__nlp__.school santa teresa', '__nlp__.school south san', '__nlp__.school wells', '__nlp__.school wells ida', '__nlp__.simulated wood', '__nlp__.upper floor washer', '__nlp__.wall furnace dishwasher', '__nlp__.webster elementary', '__nlp__.webster elementary school', '__nlp__.wells ida', '__nlp__.wells ida high', '__nlp__.you ll be', '__nlp__.garage attached master', '__nlp__.air natural gas', '__nlp__.clara high school', '__nlp__.entrance covered', '__nlp__.forced air natural', '__nlp__.garage three door', '__nlp__.santa clara high', '__nlp__.school charter wood', '__nlp__.school los gatos', '__nlp__.self cleaning', '__nlp__.self cleaning oven', '__nlp__.star qualified', '__nlp__.three door', '__nlp__.walk in pantry', '__nlp__.000 sq ft', '__nlp__.alexander hamilton senior', '__nlp__.attached ground floor', '__nlp__.auto driveway gate', '__nlp__.charter elementary school', '__nlp__.countertops stainless steel', '__nlp__.driveway gate', '__nlp__.elementary school chaboya', '__nlp__.garage attached ground', '__nlp__.golden gate park', '__nlp__.hamilton senior', '__nlp__.hamilton senior high', '__nlp__.lorenzo valley middle', '__nlp__.los feliz', '__nlp__.plan is', '__nlp__.raoul traditional', '__nlp__.raoul traditional high', '__nlp__.school ann sobrato', '__nlp__.school chaboya', '__nlp__.school chaboya middle', '__nlp__.school leigh', '__nlp__.sobrato high school', '__nlp__.traditional high', '__nlp__.traditional high school', '__nlp__.view high school', '__nlp__.wallenberg', '__nlp__.wallenberg raoul', '__nlp__.wallenberg raoul traditional', '__nlp__.westwood charter elementary', '__nlp__.with tons of', '__nlp__.ac whole house', '__nlp__.air underground basement', '__nlp__.central ac whole', '__nlp__.conditioning and', '__nlp__.forced air underground', '__nlp__.has been lovingly', '__nlp__.high end finishes', '__nlp__.hillview middle school', '__nlp__.in 2007', '__nlp__.in shelving', '__nlp__.in sought after', '__nlp__.is adjacent to', '__nlp__.la entrada', '__nlp__.mann elementary', '__nlp__.mann elementary school', '__nlp__.peterson middle', '__nlp__.rock high', '__nlp__.rock high school', '__nlp__.san cayetano', '__nlp__.school evergreen', '__nlp__.school evergreen valley', '__nlp__.school hillview', '__nlp__.school hillview middle', '__nlp__.school leland', '__nlp__.school leland high', '__nlp__.school prospect high', '__nlp__.second street elementary', '__nlp__.water heater laundry', '__nlp__.water heater microwave', '__nlp__.and lots of', '__nlp__.electricity hookup 110v', '__nlp__.hookup 110v', '__nlp__.room electricity hookup', '__nlp__.utility room electricity', '__nlp__.dorsey senior', '__nlp__.dorsey senior high', '__nlp__.hollywood senior high', '__nlp__.hookup 110v garage', '__nlp__.middle school hollywood', '__nlp__.middle school susan', '__nlp__.miller dorsey', '__nlp__.miller dorsey senior', '__nlp__.school abraham', '__nlp__.school abraham lincoln', '__nlp__.school hollywood', '__nlp__.school hollywood senior', '__nlp__.school menlo atherton', '__nlp__.school susan', '__nlp__.school susan miller', '__nlp__.standing range', '__nlp__.susan miller', '__nlp__.susan miller dorsey', '__nlp__.zones central', '__nlp__.abundant natural light', '__nlp__.bowditch middle', '__nlp__.bowditch middle school', '__nlp__.butcher block', '__nlp__.in february 2020', '__nlp__.le conte', '__nlp__.loads of', '__nlp__.los angeles ca', '__nlp__.mar elementary', '__nlp__.mar elementary school', '__nlp__.mar high', '__nlp__.mar high school', '__nlp__.revere middle', '__nlp__.revere middle school', '__nlp__.school gateway', '__nlp__.slope from street', '__nlp__.trader joe', '__nlp__.abraham high school', '__nlp__.alto high school', '__nlp__.cabrillo elementary school', '__nlp__.cabrillo middle school', '__nlp__.critical design', '__nlp__.critical design and', '__nlp__.denman james', '__nlp__.denman james middle', '__nlp__.design and gaming', '__nlp__.easy freeway access', '__nlp__.elementary school denman', '__nlp__.elementary school juan', '__nlp__.features walk in', '__nlp__.five keys independence', '__nlp__.gaming', '__nlp__.hawkins high critical', '__nlp__.high critical', '__nlp__.high critical design', '__nlp__.high school sf', '__nlp__.home is 200', '__nlp__.hood ice maker', '__nlp__.is 200 mo', '__nlp__.isaac', '__nlp__.isaac newton', '__nlp__.isaac newton graham', '__nlp__.juan cabrillo', '__nlp__.juan cabrillo middle', '__nlp__.keys independence', '__nlp__.keys independence high', '__nlp__.leadership high school', '__nlp__.lincoln abraham', '__nlp__.lincoln abraham high', '__nlp__.marina middle school', '__nlp__.middle school leadership', '__nlp__.new light fixtures', '__nlp__.newton', '__nlp__.newton graham', '__nlp__.newton graham middle', '__nlp__.palo alto high', '__nlp__.presidio middle', '__nlp__.presidio middle school', '__nlp__.range hood ice', '__nlp__.school bowditch', '__nlp__.school bowditch middle', '__nlp__.school denman', '__nlp__.school denman james', '__nlp__.school isaac', '__nlp__.school isaac newton', '__nlp__.school juan', '__nlp__.school juan cabrillo', '__nlp__.school leadership', '__nlp__.school leadership high', '__nlp__.school lincoln abraham', '__nlp__.school marina middle', '__nlp__.school presidio', '__nlp__.school presidio middle', '__nlp__.school san jose', '__nlp__.school see remarks', '__nlp__.school sf', '__nlp__.school sf sheriff', '__nlp__.school south', '__nlp__.sereno', '__nlp__.sf sheriff', '__nlp__.sheriff', '__nlp__.sink washer dryer', '__nlp__.tub sink washer', '__nlp__.winning schools', '__nlp__.wrought iron', '__nlp__.and tons of', '__nlp__.april 2020 the', '__nlp__.canyon elementary school', '__nlp__.columbia middle school', '__nlp__.detached two car', '__nlp__.dishwasher double oven', '__nlp__.echo park', '__nlp__.elementary school columbia', '__nlp__.elementary school school', '__nlp__.for advanced', '__nlp__.for advanced studies', '__nlp__.gas underground basement', '__nlp__.gateway high school', '__nlp__.george high school', '__nlp__.giannini', '__nlp__.giannini middle', '__nlp__.giannini middle school', '__nlp__.gilroy high school', '__nlp__.golden gate bridge', '__nlp__.home is 400', '__nlp__.house attic fan', '__nlp__.in april 2020', '__nlp__.is 400 mo', '__nlp__.is townhome home', '__nlp__.middle school gilroy', '__nlp__.middle school homestead', '__nlp__.palms middle', '__nlp__.palms middle school', '__nlp__.pioneer high', '__nlp__.pioneer high school', '__nlp__.pit and', '__nlp__.retreate', '__nlp__.rey middle', '__nlp__.rey middle school', '__nlp__.rooftop elementary school', '__nlp__.rosewood avenue', '__nlp__.rosewood avenue elementary', '__nlp__.school aragon', '__nlp__.school aragon high', '__nlp__.school augustus hawkins', '__nlp__.school columbia', '__nlp__.school columbia middle', '__nlp__.school five', '__nlp__.school five keys', '__nlp__.school for', '__nlp__.school for advanced', '__nlp__.school gateway high', '__nlp__.school giannini', '__nlp__.school giannini middle', '__nlp__.school gilroy', '__nlp__.school gilroy high', '__nlp__.school homestead', '__nlp__.school homestead high', '__nlp__.school palms', '__nlp__.school palms middle', '__nlp__.school paul', '__nlp__.school paul revere', '__nlp__.school pioneer high', '__nlp__.school rolling', '__nlp__.school rolling hills', '__nlp__.school school', '__nlp__.school school for', '__nlp__.school washington george', '__nlp__.spacious floor plan', '__nlp__.studies', '__nlp__.suite retreate', '__nlp__.this move in', '__nlp__.top to', '__nlp__.top to bottom', '__nlp__.townhome home', '__nlp__.townhome home that', '__nlp__.upper floor inside', '__nlp__.wall to wall', '__nlp__.washer dryer carport', '__nlp__.washington george', '__nlp__.washington george high', '__nlp__.water heater range', '__nlp__.whole house attic', '__nlp__.bay high school', '__nlp__.bedroom more than', '__nlp__.connell john', '__nlp__.connell john high', '__nlp__.driveway shared driveway', '__nlp__.dryer electricity hookup', '__nlp__.elementary school half', '__nlp__.garage faces side', '__nlp__.gas carport', '__nlp__.glassell park', '__nlp__.guest interior access', '__nlp__.in need of', '__nlp__.john high', '__nlp__.john high school', '__nlp__.languages', '__nlp__.languages magnet', '__nlp__.mark twain middle', '__nlp__.microwave self cleaning', '__nlp__.moon bay high', '__nlp__.need of', '__nlp__.opener guest interior', '__nlp__.school amador', '__nlp__.school amador high', '__nlp__.school and world', '__nlp__.school half', '__nlp__.school half moon', '__nlp__.school mark twain', '__nlp__.school palisades', '__nlp__.school palisades charter', '__nlp__.star qualified equipment', '__nlp__.twain middle', '__nlp__.twain middle school', '__nlp__.washer dryer electricity', '__nlp__.well cared for', '__nlp__.world languages', '__nlp__.world languages magnet', '__nlp__.for this house', '__nlp__.the zestimate', '__nlp__.the zestimate for', '__nlp__.coop', '__nlp__.currently used as', '__nlp__.downtown culver city', '__nlp__.downtown willow glen', '__nlp__.forced air fireplace', '__nlp__.frank greene', '__nlp__.greene', '__nlp__.greene jr', '__nlp__.greene jr middle', '__nlp__.henry gunn high', '__nlp__.home is 800', '__nlp__.is 800 mo', '__nlp__.it contains bedroom', '__nlp__.jr middle palo', '__nlp__.mann', '__nlp__.middle palo', '__nlp__.middle palo alto', '__nlp__.middle school henry', '__nlp__.mlslistings inc', '__nlp__.prospect high school', '__nlp__.pump electric', '__nlp__.rico high school', '__nlp__.rio rico high', '__nlp__.san francisco and', '__nlp__.school cupertino high', '__nlp__.school frank', '__nlp__.school frank greene', '__nlp__.school henry gunn', '__nlp__.school rio rico', '__nlp__.spa like bathroom', '__nlp__.travertine forced air', '__nlp__.washer dryer includ', '__nlp__.220 volt outlet', '__nlp__.angeles senior high', '__nlp__.buena high school', '__nlp__.coatimundi middle', '__nlp__.coatimundi middle school', '__nlp__.creek middle school', '__nlp__.driveway up slope', '__nlp__.elementary school daniel', '__nlp__.elementary school yerba', '__nlp__.garage faces rear', '__nlp__.gardens elementary school', '__nlp__.has walk in', '__nlp__.hour security', '__nlp__.https my matterport', '__nlp__.laurelwood elementary school', '__nlp__.los angeles senior', '__nlp__.marian', '__nlp__.marian peterson', '__nlp__.marian peterson middle', '__nlp__.matterport com', '__nlp__.middle school prospect', '__nlp__.moreland middle', '__nlp__.moreland middle school', '__nlp__.my matterport', '__nlp__.my matterport com', '__nlp__.opener tandem', '__nlp__.opener tandem on', '__nlp__.palisades charter high', '__nlp__.peterson middle school', '__nlp__.private independent on', '__nlp__.refrigerator built in', '__nlp__.school daniel', '__nlp__.school daniel webster', '__nlp__.school marian', '__nlp__.school marian peterson', '__nlp__.school moreland', '__nlp__.school moreland middle', '__nlp__.school thomas russell', '__nlp__.school yerba', '__nlp__.school yerba buena', '__nlp__.thomas russell', '__nlp__.thomas russell middle', '__nlp__.up slope', '__nlp__.up slope from', '__nlp__.volt outlet', '__nlp__.webster middle', '__nlp__.webster middle school', '__nlp__.william elementary school', '__nlp__.wrap around deck', '__nlp__.yerba buena high', '__nlp__.hookup 220v garage', '__nlp__.and brand new', '__nlp__.assigned controlled entrance', '__nlp__.augustus hawkins', '__nlp__.augustus hawkins high', '__nlp__.burnett middle school', '__nlp__.central garbage disposal', '__nlp__.conte middle school', '__nlp__.disposal energy star', '__nlp__.elementary school peter', '__nlp__.graham middle', '__nlp__.graham middle school', '__nlp__.hawkins', '__nlp__.hawkins high', '__nlp__.home forced air', '__nlp__.home is 700', '__nlp__.in may 2020', '__nlp__.is 000 mo', '__nlp__.is 700 mo', '__nlp__.joseph le', '__nlp__.joseph le conte', '__nlp__.le conte middle', '__nlp__.mckinley elementary school', '__nlp__.monroe middle school', '__nlp__.not disturb', '__nlp__.open concept floor', '__nlp__.peter', '__nlp__.peter burnett', '__nlp__.peter burnett middle', '__nlp__.san jose high', '__nlp__.school borel', '__nlp__.school borel middle', '__nlp__.school charter carpet', '__nlp__.school hillsdale high', '__nlp__.school jefferson high', '__nlp__.school joseph le', '__nlp__.school monroe middle', '__nlp__.school peter', '__nlp__.school peter burnett', '__nlp__.school sequoia high', '__nlp__.school solorsano', '__nlp__.school solorsano middle', '__nlp__.sequoia high', '__nlp__.sequoia high school', '__nlp__.solorsano', '__nlp__.solorsano middle', '__nlp__.solorsano middle school', '__nlp__.teresa high', '__nlp__.union high school', '__nlp__.union middle school', '__nlp__.working from home', '__nlp__.home is 500', '__nlp__.in september 2020', '__nlp__.is 500', '__nlp__.is 500 mo', '__nlp__.propane butane central', '__nlp__.and en suite', '__nlp__.attached mixed covered', '__nlp__.attached walk in', '__nlp__.christopher high school', '__nlp__.covered more than', '__nlp__.elementary school cupertino', '__nlp__.for indoor outdoor', '__nlp__.garage attached mixed', '__nlp__.garage attached walk', '__nlp__.garage electricity hookup', '__nlp__.gunn high school', '__nlp__.harte middle', '__nlp__.herbert middle', '__nlp__.herbert middle school', '__nlp__.hoover herbert', '__nlp__.hoover herbert middle', '__nlp__.in garage electricity', '__nlp__.is condition', '__nlp__.middle school christopher', '__nlp__.missed', '__nlp__.not guaranteed', '__nlp__.opener driveway', '__nlp__.opener garage', '__nlp__.school christopher', '__nlp__.school christopher high', '__nlp__.school cupertino middle', '__nlp__.school hoover', '__nlp__.school hoover herbert', '__nlp__.school monta', '__nlp__.school monta vista', '__nlp__.third street elementary', '__nlp__.water heater gas', '__nlp__.wood pellet central', '__nlp__.workshop in garage', '__nlp__.dryer range oven', '__nlp__.interior access side', '__nlp__.structure controlled', '__nlp__.structure controlled entrance', '__nlp__.attached off street', '__nlp__.basement garage attached', '__nlp__.detached off street', '__nlp__.underground basement garage', '__nlp__.bedroom walk in', '__nlp__.central ac carport', '__nlp__.culver city middle', '__nlp__.elementary school culver', '__nlp__.front garage', '__nlp__.is condo', '__nlp__.is condo home', '__nlp__.lincoln high school', '__nlp__.middle school culver', '__nlp__.school martin', '__nlp__.school oak', '__nlp__.central ac garage', '__nlp__.britton middle', '__nlp__.britton middle school', '__nlp__.butane wood pellet', '__nlp__.el camino real', '__nlp__.elementary school lewis', '__nlp__.fireplace forced air', '__nlp__.foothill high school', '__nlp__.forced air radiant', '__nlp__.george sr elementary', '__nlp__.has built in', '__nlp__.has lots of', '__nlp__.high end stainless', '__nlp__.in microwave', '__nlp__.is plenty of', '__nlp__.kitchen garage', '__nlp__.lewis britton', '__nlp__.lewis britton middle', '__nlp__.man', '__nlp__.matterport com show', '__nlp__.piedmont middle school', '__nlp__.plan the', '__nlp__.propane butane wood', '__nlp__.range hood washer', '__nlp__.school castillero', '__nlp__.school castillero middle', '__nlp__.school foothill', '__nlp__.school foothill high', '__nlp__.school lewis', '__nlp__.school lewis britton', '__nlp__.school mark', '__nlp__.school piedmont middle', '__nlp__.shirakawa', '__nlp__.shirakawa george', '__nlp__.shirakawa george sr', '__nlp__.sr elementary', '__nlp__.sr elementary school', '__nlp__.there is plenty', '__nlp__.water purifier', '__nlp__.westport heights', '__nlp__.is multi family', '__nlp__.multi family home', '__nlp__.see forced air', '__nlp__.the moment you', '__nlp__.with wet bar', '__nlp__.direct access garage', '__nlp__.school independence high', '__nlp__.scotts valley', '__nlp__.valley middle school', '__nlp__.access independent on', '__nlp__.interior access independent', '__nlp__.interior access on', '__nlp__.belmont senior high', '__nlp__.location close to', '__nlp__.rv access parking', '__nlp__.santa monica', '__nlp__.access tandem on', '__nlp__.air built in', '__nlp__.and advertised', '__nlp__.and advertised by', '__nlp__.animo charter', '__nlp__.animo charter middle', '__nlp__.appliances range hood', '__nlp__.being used as', '__nlp__.brokered', '__nlp__.brokered and', '__nlp__.brokered and advertised', '__nlp__.central air built', '__nlp__.charter middle no', '__nlp__.cochran jr middle', '__nlp__.consisting of', '__nlp__.dishwasher free standing', '__nlp__.elementary school animo', '__nlp__.elementary school johnnie', '__nlp__.fan multi', '__nlp__.forced air heating', '__nlp__.half moon bay', '__nlp__.interior access tandem', '__nlp__.is walking distance', '__nlp__.its best', '__nlp__.johnnie', '__nlp__.johnnie cochran', '__nlp__.johnnie cochran jr', '__nlp__.linda middle', '__nlp__.linda middle school', '__nlp__.middle no', '__nlp__.middle no george', '__nlp__.middle school carlmont', '__nlp__.moon bay', '__nlp__.no george', '__nlp__.no george washington', '__nlp__.offers plenty of', '__nlp__.on cul de', '__nlp__.plan offers', '__nlp__.please do not', '__nlp__.qualified appliances range', '__nlp__.quiet tree lined', '__nlp__.school animo', '__nlp__.school animo charter', '__nlp__.school el sereno', '__nlp__.school johnnie', '__nlp__.school johnnie cochran', '__nlp__.school sunnyvale', '__nlp__.school sunnyvale middle', '__nlp__.school tierra', '__nlp__.school tierra linda', '__nlp__.sereno middle', '__nlp__.sereno middle school', '__nlp__.site unassigned', '__nlp__.site unassigned condo', '__nlp__.spacious walk in', '__nlp__.sunnyvale middle school', '__nlp__.tierra linda', '__nlp__.tierra linda middle', '__nlp__.to refrigerator', '__nlp__.tolerant', '__nlp__.unassigned condo', '__nlp__.unassigned condo only', '__nlp__.water line', '__nlp__.water line to', '__nlp__.james middle school', '__nlp__.chaboya middle', '__nlp__.hookup washer hookup', '__nlp__.in closet more', '__nlp__.rio del mar', '__nlp__.school woodside high', '__nlp__.shower over tub', '__nlp__.water heater dishwasher', '__nlp__.adrian wilcox', '__nlp__.adrian wilcox high', '__nlp__.buchser middle', '__nlp__.buchser middle school', '__nlp__.central air barbecue', '__nlp__.elementary school buchser', '__nlp__.james lick high', '__nlp__.lick high', '__nlp__.lick high school', '__nlp__.other forced air', '__nlp__.room driveway', '__nlp__.school branham', '__nlp__.school branham high', '__nlp__.school buchser', '__nlp__.school buchser middle', '__nlp__.school carlmont', '__nlp__.school carlmont high', '__nlp__.school fremont', '__nlp__.school fremont high', '__nlp__.school james', '__nlp__.school james lick', '__nlp__.school roosevelt middle', '__nlp__.sold as is', '__nlp__.washer tub sink', '__nlp__.with brand new', '__nlp__.air ceiling fan', '__nlp__.propane butane', '__nlp__.tile forced air', '__nlp__.forced air elec', '__nlp__.lowell high school', '__nlp__.al fresco', '__nlp__.fresco', '__nlp__.hoover middle', '__nlp__.june 2020', '__nlp__.junior senior high', '__nlp__.opener attached', '__nlp__.quiet cul de', '__nlp__.rey', '__nlp__.roosevelt middle school', '__nlp__.santana row', '__nlp__.school english', '__nlp__.school english middle', '__nlp__.school scotts valley', '__nlp__.woodside high school', '__nlp__.alder creek middle', '__nlp__.elementary school alder', '__nlp__.elementary school beverly', '__nlp__.int access from', '__nlp__.middle school tahoe', '__nlp__.only in', '__nlp__.school alder', '__nlp__.school alder creek', '__nlp__.school beverly', '__nlp__.school beverly hills', '__nlp__.school tahoe', '__nlp__.school tahoe truckee', '__nlp__.tahoe truckee', '__nlp__.tahoe truckee high', '__nlp__.truckee high', '__nlp__.truckee high school', '__nlp__.fan central', '__nlp__.independence high school', '__nlp__.palo alto', '__nlp__.los altos', '__nlp__.an abundance of', '__nlp__.refrigerator trash compactor', '__nlp__.washer and dryer', '__nlp__.los gatos high', '__nlp__.manufactured home', '__nlp__.tops and', '__nlp__.ann sobrato high', '__nlp__.dryer refrigerator washer', '__nlp__.family and friends', '__nlp__.in july 2020', '__nlp__.in march 2020', '__nlp__.march 2020 the', '__nlp__.marina del rey', '__nlp__.milpitas high school', '__nlp__.mission high school', '__nlp__.open concept living', '__nlp__.refrigerator vented exhaust', '__nlp__.retreat more than', '__nlp__.school milpitas', '__nlp__.school milpitas high', '__nlp__.school mission high', '__nlp__.sherman elementary school', '__nlp__.sobrato', '__nlp__.sobrato high', '__nlp__.suite retreat more', '__nlp__.to detail', '__nlp__.washington preparatory', '__nlp__.washington preparatory high', '__nlp__.atherton high school', '__nlp__.city middle school', '__nlp__.is move in', '__nlp__.menlo atherton', '__nlp__.menlo atherton high', '__nlp__.school marina', '__nlp__.school hubert', '__nlp__.school hubert howe', '__nlp__.school thomas', '__nlp__.school culver city', '__nlp__.ft and was', '__nlp__.argonaut elementary school', '__nlp__.concrete forced air', '__nlp__.law unit', '__nlp__.middle school saratoga', '__nlp__.pump central', '__nlp__.saratoga high', '__nlp__.saratoga high school', '__nlp__.school saratoga', '__nlp__.school saratoga high', '__nlp__.andrew hill high', '__nlp__.detached car garage', '__nlp__.in cabinets', '__nlp__.star qualified appliances', '__nlp__.venice senior high', '__nlp__.aptos high school', '__nlp__.in bbq', '__nlp__.martin murphy middle', '__nlp__.murphy middle', '__nlp__.opener side', '__nlp__.opener side by', '__nlp__.preparatory high school', '__nlp__.school aptos high', '__nlp__.state of the', '__nlp__.no air conditioning', '__nlp__.range washer dryer', '__nlp__.redwood middle', '__nlp__.redwood middle school', '__nlp__.school connell john', '__nlp__.school redwood middle', '__nlp__.assigned community structure', '__nlp__.in garage washer', '__nlp__.lick james', '__nlp__.lick james middle', '__nlp__.mission hill middle', '__nlp__.school lick', '__nlp__.school lick james', '__nlp__.school mission hill', '__nlp__.site driveway', '__nlp__.john marshall senior', '__nlp__.marshall senior', '__nlp__.marshall senior high', '__nlp__.school john marshall', '__nlp__.thomas starr', '__nlp__.thomas starr king', '__nlp__.gas forced air', '__nlp__.than one master', '__nlp__.vented exhaust', '__nlp__.vented exhaust fan', '__nlp__.one of kind', '__nlp__.school aptos', '__nlp__.audubon middle school', '__nlp__.opener independent', '__nlp__.opener independent on', '__nlp__.school belmont', '__nlp__.school belmont senior', '__nlp__.tandem on site', '__nlp__.with lots of', '__nlp__.bancroft middle school', '__nlp__.howe bancroft', '__nlp__.howe bancroft middle', '__nlp__.hubert', '__nlp__.hubert howe', '__nlp__.hubert howe bancroft', '__nlp__.san jose ca', '__nlp__.retreat walk in', '__nlp__.suite retreat walk', '__nlp__.vinyl forced air', '__nlp__.mapped condo', '__nlp__.mapped condo only', '__nlp__.on site mapped', '__nlp__.site mapped', '__nlp__.site mapped condo', '__nlp__.hill middle school', '__nlp__.side by', '__nlp__.side by side', '__nlp__.side on', '__nlp__.side side by', '__nlp__.floor walk in', '__nlp__.ground floor walk', '__nlp__.you will find', '__nlp__.carpet forced air', '__nlp__.santa cruz', '__nlp__.mo forced air', '__nlp__.trash compactor washer', '__nlp__.contains bedrooms and', '__nlp__.covered ground floor', '__nlp__.covered walk in', '__nlp__.inside electricity hookup', '__nlp__.king middle school', '__nlp__.garage faces front', '__nlp__.lorenzo valley', '__nlp__.san lorenzo', '__nlp__.san lorenzo valley', '__nlp__.garage detached covered', '__nlp__.dishwasher garbage disposal', '__nlp__.garbage disposal range', '__nlp__.opener interior', '__nlp__.opener interior access', '__nlp__.dryer garbage disposal', '__nlp__.hookup 220v', '__nlp__.fairfax senior high', '__nlp__.on site single', '__nlp__.single family only', '__nlp__.site single', '__nlp__.site single family', '__nlp__.ground floor more', '__nlp__.freezer garbage disposal', '__nlp__.school university senior', '__nlp__.university senior', '__nlp__.university senior high', '__nlp__.garage single door', '__nlp__.two door', '__nlp__.los gatos', '__nlp__.harbor high school', '__nlp__.middle school harbor', '__nlp__.school harbor', '__nlp__.school harbor high', '__nlp__.wood forced air', '__nlp__.hardwood forced air', '__nlp__.san mateo', '__nlp__.last sold', '__nlp__.last sold for', '__nlp__.this home last', '__nlp__.that contains', '__nlp__.in november 2020', '__nlp__.november', '__nlp__.november 2020', '__nlp__.independent on site', '__nlp__.laminate forced air', '__nlp__.linoleum forced air', '__nlp__.suite retreat', '__nlp__.rent zestimate for', '__nlp__.the rent zestimate', '__nlp__.school fairfax senior', '__nlp__.school forced air', '__nlp__.the heart of', '__nlp__.zestimate for this']\n",
            "\t\t\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', [])                    :    3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t('category', ['text_as_category'])  :   15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\t\t('float', [])                       :   17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])                         :    2 | ['Id', 'Zip']\n",
            "\t\t\t\t('int', ['binned', 'text_special']) :  179 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t\t\t('int', ['bool'])                   :    1 | ['State']\n",
            "\t\t\t\t('int', ['datetime_as_int'])        :   10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "\t\t\t\t('int', ['text_ngram'])             : 8909 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in august', ...]\n",
            "\t\t\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t\t\t('category', [])                    :    3 | ['Type', 'Region', 'City']\n",
            "\t\t\t\t('category', ['text_as_category'])  :   15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t\t\t('float', [])                       :   17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t\t\t('int', [])                         :    2 | ['Id', 'Zip']\n",
            "\t\t\t\t('int', ['binned', 'text_special']) :  179 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t\t\t('int', ['bool'])                   :    1 | ['State']\n",
            "\t\t\t\t('int', ['datetime_as_int'])        :   10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "\t\t\t\t('int', ['text_ngram'])             : 8909 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in august', ...]\n",
            "\t\t\t36.3s = Fit runtime\n",
            "\t\t\t9136 features in original data used to generate 9136 features in processed data.\n",
            "\tTypes of features in original data (exact raw dtype, raw dtype):\n",
            "\t\t('float64', 'float') : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('int64', 'int')     :  2 | ['Id', 'Zip']\n",
            "\t\t('object', 'object') : 21 | ['Address', 'Summary', 'Type', 'Heating', 'Cooling', ...]\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])                      : 17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('int', [])                        :  2 | ['Id', 'Zip']\n",
            "\t\t('object', [])                     :  4 | ['Type', 'Region', 'City', 'State']\n",
            "\t\t('object', ['datetime_as_object']) :  2 | ['Listed On', 'Last Sold On']\n",
            "\t\t('object', ['text'])               : 15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\tTypes of features in processed data (exact raw dtype, raw dtype):\n",
            "\t\t('category', 'category') :   18 | ['Address', 'Summary', 'Type', 'Heating', 'Cooling', ...]\n",
            "\t\t('float64', 'float')     :   17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('int64', 'int')         :   12 | ['Id', 'Zip', 'Listed On', 'Listed On.year', 'Listed On.month', ...]\n",
            "\t\t('int8', 'int')          :    1 | ['State']\n",
            "\t\t('uint16', 'int')        : 8909 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in august', ...]\n",
            "\t\t('uint8', 'int')         :  179 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', [])                    :    3 | ['Type', 'Region', 'City']\n",
            "\t\t('category', ['text_as_category'])  :   15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "\t\t('float', [])                       :   17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "\t\t('int', [])                         :    2 | ['Id', 'Zip']\n",
            "\t\t('int', ['binned', 'text_special']) :  179 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "\t\t('int', ['bool'])                   :    1 | ['State']\n",
            "\t\t('int', ['datetime_as_int'])        :   10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "\t\t('int', ['text_ngram'])             : 8909 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in august', ...]\n",
            "\t144.0s = Fit runtime\n",
            "\t40 features in original data used to generate 9136 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 826.30 MB (1.6% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 150.7s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Saving /content/california-house-prices/AutoGluonModels/learner.pkl\n",
            "Automatically generating train/validation split with holdout_frac=0.05269925588650688, Train Rows: 44939, Val Rows: 2500\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}],\n",
            "\t'XGB': [{}],\n",
            "\t'FASTAI': [{}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "}\n",
            "Saving /content/california-house-prices/AutoGluonModels/utils/data/X.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/utils/data/y.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/utils/data/X_val.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/utils/data/y_val.pkl\n",
            "Model configs that will be trained (in order):\n",
            "\tLightGBMXT: \t{'extra_trees': True, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'name_suffix': 'XT', 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
            "\tLightGBM: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>, 'priority': 90}}\n",
            "\tRandomForestMSE: \t{'criterion': 'squared_error', 'ag_args': {'problem_types': ['regression', 'quantile'], 'name_suffix': 'MSE', 'model_type': <class 'autogluon.tabular.models.rf.rf_model.RFModel'>, 'priority': 80}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
            "\tCatBoost: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'model_type': <class 'autogluon.tabular.models.catboost.catboost_model.CatBoostModel'>, 'priority': 70}}\n",
            "\tExtraTreesMSE: \t{'criterion': 'squared_error', 'ag_args': {'problem_types': ['regression', 'quantile'], 'name_suffix': 'MSE', 'model_type': <class 'autogluon.tabular.models.xt.xt_model.XTModel'>, 'priority': 60}, 'ag_args_ensemble': {'use_child_oof': True}}\n",
            "\tNeuralNetFastAI: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile'], 'model_type': <class 'autogluon.tabular.models.fastainn.tabular_nn_fastai.NNFastAiTabularModel'>, 'priority': 50}}\n",
            "\tXGBoost: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'softclass'], 'model_type': <class 'autogluon.tabular.models.xgboost.xgboost_model.XGBoostModel'>, 'priority': 40}}\n",
            "\tNeuralNetTorch: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'model_type': <class 'autogluon.tabular.models.tabular_nn.torch.tabular_nn_torch.TabularNeuralNetTorchModel'>, 'priority': 25}}\n",
            "\tLightGBMLarge: \t{'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None, 'model_type': <class 'autogluon.tabular.models.lgb.lgb_model.LGBModel'>}}\n",
            "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: LightGBMXT ... Training model for up to 449.30s of the 449.29s of remaining time.\n",
            "\tFitting LightGBMXT with 'num_gpus': 0, 'num_cpus': 6\n",
            "\tFitting with cpus=6, gpus=0, mem=5.6/48.8 GB\n",
            "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05, 'extra_trees': True}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalid_set's rmse: 500535\n",
            "[100]\tvalid_set's rmse: 432890\n",
            "[150]\tvalid_set's rmse: 410792\n",
            "[200]\tvalid_set's rmse: 398594\n",
            "[250]\tvalid_set's rmse: 395181\n",
            "[300]\tvalid_set's rmse: 389588\n",
            "[350]\tvalid_set's rmse: 392043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving /content/california-house-prices/AutoGluonModels/models/LightGBMXT/model.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/utils/attr/LightGBMXT/y_pred_proba_val.pkl\n",
            "\t-389418.2663\t = Validation score   (-root_mean_squared_error)\n",
            "\t27.57s\t = Training   runtime\n",
            "\t0.12s\t = Validation runtime\n",
            "\t20993.4\t = Inference  throughput (rows/s | 2500 batch size)\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Fitting model: LightGBM ... Training model for up to 421.59s of the 421.58s of remaining time.\n",
            "\tFitting LightGBM with 'num_gpus': 0, 'num_cpus': 6\n",
            "\tFitting with cpus=6, gpus=0, mem=5.6/48.6 GB\n",
            "\tFitting 10000 rounds... Hyperparameters: {'learning_rate': 0.05}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50]\tvalid_set's rmse: 333904\n",
            "[100]\tvalid_set's rmse: 362304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving /content/california-house-prices/AutoGluonModels/models/LightGBM/model.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/utils/attr/LightGBM/y_pred_proba_val.pkl\n",
            "\t-327695.8952\t = Validation score   (-root_mean_squared_error)\n",
            "\t20.74s\t = Training   runtime\n",
            "\t0.09s\t = Validation runtime\n",
            "\t26630.6\t = Inference  throughput (rows/s | 2500 batch size)\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Fitting model: RandomForestMSE ... Training model for up to 400.74s of the 400.73s of remaining time.\n",
            "\tFitting RandomForestMSE with 'num_gpus': 0, 'num_cpus': 12\n",
            "\tFitting with cpus=12, gpus=0, mem=0.0/48.5 GB\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/RandomForestMSE/model.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/utils/attr/RandomForestMSE/y_pred_proba_val.pkl\n",
            "\t-361194.9404\t = Validation score   (-root_mean_squared_error)\n",
            "\t4816.53s\t = Training   runtime\n",
            "\t0.19s\t = Validation runtime\n",
            "\t13365.2\t = Inference  throughput (rows/s | 2500 batch size)\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Skipping CatBoost due to lack of time remaining.\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Skipping ExtraTreesMSE due to lack of time remaining.\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Skipping NeuralNetFastAI due to lack of time remaining.\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Skipping XGBoost due to lack of time remaining.\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Skipping NeuralNetTorch due to lack of time remaining.\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Skipping LightGBMLarge due to lack of time remaining.\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/utils/attr/LightGBM/y_pred_proba_val.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/utils/attr/LightGBMXT/y_pred_proba_val.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/utils/attr/RandomForestMSE/y_pred_proba_val.pkl\n",
            "Model configs that will be trained (in order):\n",
            "\tWeightedEnsemble_L2: \t{'ag_args': {'problem_types': ['binary', 'multiclass', 'regression', 'quantile', 'softclass'], 'valid_base': False, 'name_bag_suffix': '', 'model_type': <class 'autogluon.core.models.greedy_ensemble.greedy_weighted_ensemble_model.GreedyWeightedEnsembleModel'>, 'priority': 0}, 'ag_args_ensemble': {'save_bag_folds': True}}\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -4416.76s of remaining time.\n",
            "\tFitting WeightedEnsemble_L2 with 'num_gpus': 0, 'num_cpus': 12\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/models/WeightedEnsemble_L2/utils/model_template.pkl\n",
            "Ensemble size: 20\n",
            "Ensemble weights: \n",
            "[0.35 0.45 0.2 ]\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/WeightedEnsemble_L2/utils/oof.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/WeightedEnsemble_L2/model.pkl\n",
            "\tEnsemble Weights: {'LightGBM': 0.45, 'LightGBMXT': 0.35, 'RandomForestMSE': 0.2}\n",
            "\t-281822.3753\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.01s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "\t6240.4\t = Inference  throughput (rows/s | 2500 batch size)\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "AutoGluon training complete, total runtime = 5019.93s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 6240.4 rows/s (2500 batch size)\n",
            "Loading: /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/models/trainer.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/learner.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/predictor.pkl\n",
            "Saving /content/california-house-prices/AutoGluonModels/version.txt with contents \"1.4.0\"\n",
            "Saving /content/california-house-prices/AutoGluonModels/metadata.json\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/california-house-prices/AutoGluonModels\")\n",
            "Loading: /content/california-house-prices/AutoGluonModels/models/LightGBMXT/model.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/models/LightGBM/model.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/models/RandomForestMSE/model.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/models/WeightedEnsemble_L2/model.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                 model      score_val              eval_metric  pred_time_val     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0  WeightedEnsemble_L2 -281822.375330  root_mean_squared_error       0.400613  4864.851567                0.000598           0.011277            2       True          4\n",
            "1             LightGBM -327695.895205  root_mean_squared_error       0.093877    20.744567                0.093877          20.744567            1       True          2\n",
            "2      RandomForestMSE -361194.940385  root_mean_squared_error       0.187053  4816.529906                0.187053        4816.529906            1       True          3\n",
            "3           LightGBMXT -389418.266346  root_mean_squared_error       0.119085    27.565818                0.119085          27.565818            1       True          1\n",
            "Number of models trained: 4\n",
            "Types of models trained:\n",
            "{'WeightedEnsembleModel', 'RFModel', 'LGBModel'}\n",
            "Bagging used: False \n",
            "Multi-layer stack-ensembling used: False \n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', [])                    :    3 | ['Type', 'Region', 'City']\n",
            "('category', ['text_as_category'])  :   15 | ['Address', 'Summary', 'Heating', 'Cooling', 'Parking', ...]\n",
            "('float', [])                       :   17 | ['Year built', 'Lot', 'Bathrooms', 'Full bathrooms', 'Total interior livable area', ...]\n",
            "('int', [])                         :    2 | ['Id', 'Zip']\n",
            "('int', ['binned', 'text_special']) :  179 | ['Address.char_count', 'Address.word_count', 'Address.capital_ratio', 'Address.lower_ratio', 'Address.digit_ratio', ...]\n",
            "('int', ['bool'])                   :    1 | ['State']\n",
            "('int', ['datetime_as_int'])        :   10 | ['Listed On', 'Listed On.year', 'Listed On.month', 'Listed On.day', 'Listed On.dayofweek', ...]\n",
            "('int', ['text_ngram'])             : 8909 | ['__nlp__.00', '__nlp__.000', '__nlp__.000 in', '__nlp__.000 in april', '__nlp__.000 in august', ...]\n",
            "Plot summary of models saved to file: /content/california-house-prices/AutoGluonModels/SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        }
      ],
      "source": [
        "predictor = TabularPredictor(label=label, eval_metric=eval_metric, path=save_path, verbosity=3).fit(\n",
        "    train_data, presets='medium_quality', time_limit=600)\n",
        "\n",
        "results = predictor.fit_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sfUGO8ujMrO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "44da3a05-4b6a-4fd3-983a-0eb8c1d68356"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading: /content/california-house-prices/AutoGluonModels/models/LightGBM/model.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/models/LightGBMXT/model.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/models/RandomForestMSE/model.pkl\n",
            "Loading: /content/california-house-prices/AutoGluonModels/models/WeightedEnsemble_L2/model.pkl\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    7.991716e+05\n",
              "1    5.807125e+05\n",
              "2    8.373142e+05\n",
              "3    8.081618e+05\n",
              "4    1.166623e+06\n",
              "Name: Sold Price, dtype: float32"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sold Price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.991716e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.807125e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.373142e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.081618e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.166623e+06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float32</label>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# test_identity = pd.read_csv(directory+'test_identity.csv')\n",
        "# test_transaction = pd.read_csv(directory+'test_transaction.csv')\n",
        "# test_data = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')  # same join applied to training files\n",
        "\n",
        "y_predproba = predictor.predict(test_data)\n",
        "y_predproba.head(5)  # some example predicted fraud-probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxa-7IcgMuen"
      },
      "source": [
        "**dtype**: float32\n",
        "\n",
        "When submitting predicted probabilities for classification competitions, it is imperative these correspond to the same class expected by Kaggle. For binary classification tasks, you can see which class AutoGluon's predicted probabilities correspond to via:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hKMDD42jM1Yi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774052e8-ee3a-49ba-d289-cbdbb30fcb8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: Attempted to retrieve positive class label in a non-binary problem. Positive class labels only exist in binary classification. Returning None instead. self.problem_type is 'regression' but positive_class only exists for 'binary'.\n"
          ]
        }
      ],
      "source": [
        "predictor.positive_class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTAJlwwhM36C"
      },
      "source": [
        "For multiclass classification tasks, you can see which classes AutoGluon's predicted probabilities correspond to via:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "c7VJSfyqM5r2"
      },
      "outputs": [],
      "source": [
        "predictor.class_labels  # classes in this list correspond to columns of predict_proba() output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVea_XZGM7OQ"
      },
      "source": [
        "Now, let's get prediction probabilities for the entire test data, while only getting the positive class predictions by specifying:\n",
        "\n",
        "Now that we have made a prediction for each row in the test dataset, we can submit these predictions to Kaggle. Most Kaggle competitions provide a sample submission file, in which you can simply overwrite the sample predictions with your own as we do below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kE3gnjrmM9MS"
      },
      "outputs": [],
      "source": [
        "submission = pd.read_csv(directory+'sample_submission.csv')\n",
        "submission['Sold Price'] = y_predproba\n",
        "submission.head()\n",
        "submission.to_csv(directory+'my_submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qm6atZNtND_-"
      },
      "source": [
        "We have now completed steps (4)-(6) from the top of this tutorial. To submit your predictions to Kaggle, you can run the following command in your terminal (from the appropriate directory):\n",
        "\n",
        "`kaggle competitions submit -c ieee-fraud-detection -f sample_submission.csv -m \"my first submission\"`\n",
        "\n",
        "You can now play with different `fit()` arguments and feature-engineering techniques to try and maximize the rank of your submissions in the Kaggle Leaderboard!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWuFwi6oNM1H"
      },
      "source": [
        "**Tips to maximize predictive performance:**\n",
        "\n",
        "*   Be sure to specify the appropriate evaluation metric if one is specified on the competition website! If you are unsure which metric is best, then simply do not specify this argument when invoking `fit()`; AutoGluon should still produce high-quality models by automatically inferring which metric to use.\n",
        "*   If the training examples are time-based and the competition test examples come from future data, we recommend you reserve the most recently-collected training examples as a separate validation dataset passed to `fit()`. Otherwise, you do not need to specify a validation set yourself and AutoGluon will automatically partition the competition training data into its own training/validation sets.\n",
        "* Beyond simply specifying `presets = 'best_quality'`, you may play with more advanced `fit()` arguments such as: `num_bag_folds`, `num_stack_levels`, `num_bag_sets`, `hyperparameter_tune_kwargs`, `hyperparameters`, `refit_full`. However we recommend spending most of your time on feature-engineering and just specify presets = `'best_quality'` inside the call to `fit()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7txKwKNs5x"
      },
      "source": [
        "**Troubleshooting:**\n",
        "\n",
        "\n",
        "*   Check that you have the right user-permissions on your computer to access the data files downloaded from Kaggle.\n",
        "*   For issues downloading Kaggle data or submitting predictions, check your Kaggle account setup and the Kaggle FAQ.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN5PqP3PpwYs/IgmM/vVlan",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}